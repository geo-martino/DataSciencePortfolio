{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Work in Progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommending Top-N movies - Part 2: Testing recommendation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 1 of this project, we looked at the MovieLens data set and implemented algorithms to recommend movies based off of this. We took a brief look at some of these algorithms parameters and differences in runtime. In this part of the project, we will use various metrics to test the accuracy and validity of the recommended movies using a train-test-validation split of the reduced data set.\n",
    "\n",
    "To do this we'll use three new classes: SplitData, Metrics, and Tester which split the ratings pivot data frame, run metrics on the recommendations, and run tests over multiple users respectively. More documentation for these classes can be found in the MovieLensData.py and Tester.py files, with further information on the algorithms in the Algorithms.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "- Creating the ratings pivot data frame\n",
    "- Splitting the data for testing\n",
    "- Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing the required packages and building the ratings pivot data frame from a reduced set of data. As discussed in the part 1, we're sampling users with rating counts between 200-6000 and limiting movies to those with more than 100 ratings to avoid biases from outliers and to work within the constraints of computing power available. However, as the tests are very computationally intensive, I will initially be reducing this further by taking a random sample of 10% of the reduced userIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import importlib\n",
    "import random\n",
    "import MovieLensData\n",
    "import Algorithms\n",
    "import Tester\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading..."
     ]
    }
   ],
   "source": [
    "ml = MovieLensData.MovieLensData()\n",
    "userIDs = ml.filterIDs('userId', minRatings=200, maxRatings=6000)\n",
    "movieIDs = ml.filterIDs('movieId', minRatings=100)\n",
    "userIDs = random.sample(userIDs, int(0.1 * len(userIDs)))\n",
    "ml.reduce(userIDs, 'userId', 'ratings')\n",
    "ml.reduce(movieIDs, 'movieId', 'movies')\n",
    "\n",
    "ratingsData = ml.buildPivot(printStats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SplitData class splits up the ratings pivot data frame into a trainSet and testSet_full. testSet_full is then split in two ways: one into a a test-validation split where every user in testSet_full has a percentage or number of ratings split off into the validation set, and one that takes away just one rating from each user for leave-one-out cross validation (LOO-CV). The size of testSet_full and validation sets are governed by the testSize and validationSize parameters respectively. As the data is relatively large, we afford to keep more users in the train set, so 20% splits will be used for both the testSize and validationSize parameters.\n",
    "\n",
    "The ratings in the test set are used to generate recommendations, with the two validation sets used to check the quality of recommendations against various metrics. We can ensure the same splits occur each time via the randomState parameter for consistency in results reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, validation, LOO_test, LOO_dropped = MovieLensData.SplitData(ratingsData).buildAll(testSize=0.2, \n",
    "                                                                             validationSize=0.2, randomState=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the previous part, we can easily build each model by calling the buildModel method in the Algorithm class. This time, we initialise the method by passing the train and test set data so the program knows which set to build the model from, and which to use to look up ratings for testing. We will also need to call the buildMatrix method to generate the sparse matrix for building the KNN models.\n",
    "\n",
    "However, this time we will not need to generate a similarity matrix for the users. As these models are built off of the train set and we will not be testing any f the userIDs in this set, we do not need to build a user similarity matrix for the CF algorithm and can simply pass 'None' as the model parameter when testing this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algo = Algorithms.Algorithms(ml, train, test)\n",
    "ratingsSparse = algo.buildMatrix()\n",
    "printStatus = True\n",
    "\n",
    "CF_itemModel = algo.buildModel(modelType='CF', matrix='item', printStatus=printStatus)\n",
    "KNN_itemModel = algo.buildModel(modelType='KNN', matrix=ratingsSparse.transpose(), printStatus=printStatus)\n",
    "KNN_userModel = algo.buildModel(modelType='KNN', matrix=ratingsSparse, printStatus=printStatus)\n",
    "#SVD_model = algo.buildModel(modelType='SVD', printStatus=printStatus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Demonstration and Metrics\n",
    "\n",
    "- Adding algorithms for testing\n",
    "- Running basic and parameter tests\n",
    "- MAE, RMSE, Coverage, Diversity, Novelty, Hit Rates (Validation, LOO-CV, Cumulative, Mean Reciprocal, Actual Rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Tester class\n",
    "\n",
    "The Metrics class contains the various metrics we'll be using the test the quality of our results. This takes the validation and LOO-CV sets to check the results against, a filename for the csv that stores the results, as well as various parameters for testing (discussed below).\n",
    "\n",
    "This object is then passed to the Tester class to create our test object. This object has two primary methods for testing, runBasicTest and runParameterTest, as well as two methods for adding and removin algorithms. All we need to do to set up tests is to instantiate the tester object, and pass the various algorithms' names, method to call, model and all other parameters to control for this test. The parameters we control will be discussed in greater detail when discussing results later in this notebook.\n",
    "\n",
    "Here we'll also add a random control algorithm which simply generates random similarity scores and random rating precitions for every user. The randomRatings parameter of this method determines whether or not the rating predictions are a random number between 0-5 independent of the similarity scores, or if the rating predictions are a multiple of the randomly generated similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = 'ML_MetricsData'\n",
    "evaluator = Tester.Metrics(validation, LOO_dropped, topN=10, moviesPerPage=5, thresholdRating=3.0, csvName=filename)\n",
    "tester = Tester.Tester(evaluator)\n",
    "\n",
    "neighbours = 100\n",
    "sample = 100\n",
    "thresh = 2.0\n",
    "\n",
    "tester.addAlgorithm('Item-Based CF', algo.itemBased, model=CF_itemModel, modelType='CF', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')\n",
    "tester.addAlgorithm('User-Based CF', algo.userBased, model=None, modelType='CF', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')\n",
    "tester.addAlgorithm('Item-Based KNN', algo.itemBased, model=KNN_itemModel, modelType='KNN', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')\n",
    "tester.addAlgorithm('User-Based KNN', algo.userBased, model=KNN_userModel, modelType='KNN', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')\n",
    "#tester.addAlgorithm('SVD', algo.SVD, model=SVD_model, sample=sample, pred='calc')\n",
    "tester.addAlgorithm('Random Control', algo.random, randomRatings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the algorithms loaded, we can now run a simple test using the test set. The runBasicTest takes a few key arguements: \n",
    "- testData - the test set\n",
    "- testAlgo - the name of the algorithm from the dictionary of loaded algorithms to test \n",
    "- sampleTest - the number of users from the test to sample\n",
    "- param, pValue - the paramater and its value to modify from the stored parameters within the object\n",
    "\n",
    "Here we run a simply test on the above using the stored algorithms for 10 randomly sample users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Random Control:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tester.runBasicTest(test, sampleTest=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeaveOneOut cross validation metric uses a different set of ratings per user and is therefore needs to be identified when initialising tests. We can define this by setting LOOCV to True. This usually defaults to False, testing the results against the validation HitRate metric instead (metric types discussed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Random Control:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tester.runBasicTest(LOO_test, LOOCV=True, sampleTest=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runParameterTest method allows us to iterate through a range of values for a given parameter. The parameter is passed to the runBasicTest to run tests across the users present in the test set. This allows us to see the effect of changing parameters within the methods, with the aim of fine-tuning the algorithms. The parameters for this function are much the same as the runBasicTest method, with pValue now replaced with pRange as the range of parameters to test.\n",
    "\n",
    "Here we run a basic demonstration on the 'pred' parameter which changes the rating prediction algorithm (discussed in greater in the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parameter Testing:   0%|          | 0/4 [00:00<?, ?Parameter/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Random Control:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Random Control:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Random Control:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based CF:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Item-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing User-Based KNN:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Random Control:   0%|          | 0/10 [00:00<?, ? Users/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameter = 'predict'\n",
    "pRange = ['rand', 'calc', 'sims', 'norm_sims']\n",
    "sampleTest = 10\n",
    "\n",
    "tester.runParameterTest(test, param=parameter, pRange=pRange, sampleTest=sampleTest, printResults=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the readCSV method in the evaluator class, we can see the results we have just generated ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-90b4262ed90f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadCSV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator.readCSV(filename).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and the Metrics class\n",
    "\n",
    "The recommendations for each userID are passed to the an instantiate object of the Metrics class for evaluation. It runs various tests and saves the results to a csv file for later analysis. It requires a few parameters upon initialisation to run the tests: \n",
    "- Top-N: the top number of recommendations to consider (used by all metrics bar coverage)\n",
    "- moviesPerPage: the number of movies that will appear per page for the end-user, used only by the meanReciprocalHR metric)\n",
    "- thresholdRating: the minimum rating to be considered by the cumulativeHR metric\n",
    "\n",
    "Below is a full list and description of the metrics used for testing.\n",
    "\n",
    "#### MAE\n",
    "\n",
    "Error on the predicted ratings from actual ratings. Calculated by taking the absolute difference between the predicted and actual ratings for every hit and finding the mean of these errors. Can range from 0 to infinite. Lower values show a more accurate set of rating predictions.\n",
    "\n",
    "#### RMSE\n",
    "\n",
    "Error on the predicted ratings from actual ratings. RMSE gives a higher weight to larger errors and an indication of the increase in variance of the frequency distribution of error magnitudes. Due to this, it will always be equal to or greater than MAE.\n",
    "\n",
    "RMSE is found by squaring the difference between predicted and actual ratings for hits, finding the mean of these and finally square rooting. Can range from 0 to infinite. Lower values show a more accurate set of rating predictions, with values similar to the MAE value showing less variation of the distribution of errors.\n",
    "\n",
    "#### Coverage\n",
    "\n",
    "The number of total movies from the data set the algorithm is able to recommend to the user. Ignores Top-N, dividing the total number of movies in the results by the movies available in the training set. Expressed as a percentage in decimal form ranging from 0 to 1.\n",
    "\n",
    "Higher coverage values are preferred to ensure the algorithm is accessing the full extent of the items available.\n",
    "\n",
    "#### Diversity\n",
    "\n",
    "How diverse the Top-N recommendations are i.e., are the Top-N movies very closely related (low diversity) or not (high diversity). Found finding the mean of the similarities of the Top-N movies (S) and subtracting from 1 (1-S). Expressed as a percentage in decimal form ranging from 0 to 1.\n",
    "\n",
    "Very diverse recommendations are more likely to engage the user. Less diverse recommendations may just be recommending very closely related movies, like sequels of movies that the user is most likely to have already seen.\n",
    "\n",
    "#### Novelty\n",
    "\n",
    "How novel the Top-N movies are i.e., how much of the long-tail of less popular movies is the algorithm able to recommend in the Top-N. Found by dividing the mean of the ranks of the Top-N movies by the lowest ranked movie in the training set. Expressed as a percentage in decimal form ranging from 0 to 1. \n",
    "\n",
    "Higher values are usually preferred however, very high values may cause a lack of trust in the recommendations from the user and cause them to disengage with the recommendation list.\n",
    "\n",
    "#### Validation HitRate\n",
    "\n",
    "How many movies from the Top-N recommendations are in the validation set. Simply the number of the movies from the Top-N in the validation set divided by the number of Top-N movies. Expressed as a percentage in decimal form ranging from 0 to 1. Higher values show more hits per user.\n",
    "\n",
    "#### LeaveOneOut-CrossValidation HitRate\n",
    "\n",
    "The number of times the algorithm recommends the movie left out for LOO-CV in the Top-N recommendations. Either 0 or 1 for every user. Higher values show more hits per user.\n",
    "\n",
    "Difficult to achieve as the algorithm could recommend many movies the user likes, but miss the one movie left out for this validation set. Useful for smaller data sets where training and test data is limited.\n",
    "\n",
    "#### Cumulative HitRate\n",
    "\n",
    "Exactly the same as validation hit-rate however, we now limit the hits to only hits with a predicted rating above some threshold rating. This is to avoid giving the algorithm a positive result even if its recommendations are movies it thinks the user won't actually like.\n",
    "\n",
    "#### Mean Reciprocal HitRate\n",
    "\n",
    "Weights the Top-N hits by which page they appear on for a paginated recommendation system. Works just like the validation hit-rate, but now splits up hits into groups of ranks, dividing by the group number (or page) it is in. It then sums these together to give the hit-rate. Higher values show more weighted hits per user.\n",
    "\n",
    "This is useful for system like Netflix where 5 movies are shown to a page. Our algorithm shouldn't be rewarded as highly for only recommending movies the user actually wants to watch on the last page where the user may not even see them.\n",
    "\n",
    "#### Actual Rating HitRate\n",
    "\n",
    "A breakdown of the hits per predicted rating given. The distribution shows us whether the algorithm tends to recommend movies it thinks the user will rate highly or not. The values can be normalised for the total movies available for each user in the validation set showing how many of the ratio for each rating it is able to correctly recommend.\n",
    "\n",
    "A mean value can be extracted from these to show which ratings the average rating given to the hits of the Top-N recommendations. Higher values will therefore be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "- Basic results\n",
    "- Testing the rating prediction paramater\n",
    "- Testing the threshold rating parameter\n",
    "- Testing the sample parameter for item-based algorithms\n",
    "- Testing the sample parameter for user-based algorithms\n",
    "- Testing the neighbours parameter for item-based algorithms\n",
    "- Testing the neighbours parameter for user-based algorithms\n",
    "- Runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LOO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0cdaa8247c27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ML_MetricsData'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTester\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoviesPerPage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholdRating\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsvName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtester\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTester\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTester\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mneighbours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LOO' is not defined"
     ]
    }
   ],
   "source": [
    "filename = 'ML_MetricsData'\n",
    "evaluator = Tester.Metrics(validation, LOO_dropped, topN=30, moviesPerPage=5, thresholdRating=3.0, csvName=filename)\n",
    "tester = Tester.Tester(evaluator)\n",
    "\n",
    "neighbours = 100\n",
    "sample = 100\n",
    "thresh = 2.0\n",
    "\n",
    "tester.addAlgorithm('Item-Based CF', algo.itemBased, model=CF_itemModel, modelType='CF', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')\n",
    "tester.addAlgorithm('User-Based CF', algo.userBased, model=None, modelType='CF', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')\n",
    "tester.addAlgorithm('Item-Based KNN', algo.itemBased, model=KNN_itemModel, modelType='KNN', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')\n",
    "tester.addAlgorithm('User-Based KNN', algo.userBased, model=KNN_userModel, modelType='KNN', neighbours=neighbours,\n",
    "                    sample=sample, threshold=thresh, predict='calc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.addAlgorithm('Random Control', algo.random, randomRatings=True)\n",
    "tester.runBasicTest(test, testAlgo='Random Control')\n",
    "test.removeAlgorithm('Random Control')\n",
    "\n",
    "tester.addAlgorithm('Random Control', algo.random, randomRatings=False)\n",
    "tester.runBasicTest(test, testAlgo='Random Control')\n",
    "test.removeAlgorithm('Random Control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'neighbours'\n",
    "pRange = np.arange(10, 100, 10)\n",
    "\n",
    "tester.runParameterTest(test, testAlgo='Item-Based KNN', param=parameter, pRange=[80, 90], printResults=False)\n",
    "tester.runParameterTest(test, testAlgo='User-Based CF', param=parameter, pRange=pRange, printResults=False)\n",
    "tester.runParameterTest(test, testAlgo='User-Based KNN', param=parameter, pRange=pRange, printResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'predict'\n",
    "pRange = ['calc', 'rand', 'sims', 'norm_sims']\n",
    "\n",
    "tester.runParameterTest(test, param=parameter, pRange=pRange, printResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'sample'\n",
    "pRange = np.arange(100, 6050, 50)\n",
    "\n",
    "tester.runParameterTest(test, testAlgo='Item-Based CF', param=parameter, pRange=pRange, printResults=False)\n",
    "#tester.runParameterTest(test, testAlgo='Item-Based KNN', param=parameter, pRange=pRange, printResults=False)\n",
    "tester.runParameterTest(test, testAlgo='User-Based CF', param=parameter, pRange=pRange, printResults=False)\n",
    "tester.runParameterTest(test, testAlgo='User-Based KNN', param=parameter, pRange=pRange, printResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'neighbours'\n",
    "pRange = np.arange(100, len(train.columns), 50)\n",
    "\n",
    "tester.runParameterTest(test, testAlgo='Item-Based CF', param=parameter, pRange=pRange, printResults=False)\n",
    "#tester.runParameterTest(test, testAlgo='Item-Based KNN', param=parameter, pRange=pRange, printResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'neighbours'\n",
    "pRange = np.arange(100, len(train.index), 200)\n",
    "\n",
    "tester.runParameterTest(test, testAlgo='User-Based CF', param=parameter, pRange=pRange, printResults=False)\n",
    "tester.runParameterTest(test, testAlgo='User-Based KNN', param=parameter, pRange=pRange, printResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discussion\n",
    "- ReRun basic test for LOO-CV at end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
